## Lesson 4 - Introduction to PyTorch 

### Tensor 
- A tensor is a generalisation of vactors and matrices.   
- PyTorch tensors are pretty much the same as Numpy arrays. They come with some nice benefits such as GPU acceleration.  
- The number of hidden units a parameter of the network, often called a hyperparameter to differentiate it from the weights and biases parameters. 

### PyTorch and NumPy arrays 
- PyTorch has a great feature for converting between Numpy arrays and Torch tensors. To create a tensor from a Numpy array, use torch.from_numpy(). To convert a tensor to a Numpy array, use the .numpy() method.
- The memory is shared between the Numpy array and Torch tensor, so if you change the values in-place of one object, the other will change as well.

### PyTorch Neural Networks 
- It is mandatory to inherit from nn.Module when you're creating a class for your network. The name of the class itself can be anything. 
- You can access the weight and bias tensors once the network once it's create at net.hidden.weight and net.hidden.bias.
- You'll need to sequence the operations correctly in the forward method.
- To make sure PyTorch keeps track of operations on a tensor and calculates the gradients, you need to set requires_grad = True on a tensor.
- When we create a network with PyTorch, all of the parameters are initialized with requires_grad = True. 
- During training we want to use dropout to prevent overfitting, but during inference we want to use the entire network. So, we need to turn off dropout during validation, testing, and whenever we're using the network to make predictions. 
- The parameters for PyTorch networks are stored in a model's state_dict. We can see the state dict contains the weight and bias matrices for each of our layers. 
- Loading the state dict works only if the model architecture is exactly the same as the checkpoint architecture. 